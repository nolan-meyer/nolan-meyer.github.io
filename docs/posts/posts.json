[
  {
    "path": "posts/Stat452_TS_miniProj/",
    "title": "Time Series Mini Project",
    "description": "Modeling Google Search Trend Data: MLB",
    "author": [],
    "date": "2022-03-01",
    "categories": [],
    "contents": "\nAuthors: Nolan Meyer & Declan Elias\nIntroduction\nBaseball has a pastime rooted in American sports history, and has been one of the most popular sports in America for many generations. The first season began in 1901, and the popularity has grown ever since to a point where baseball stars have become household names across the country. The MLB – Major League Baseball – is the top baseball league in the US where each team plays 162 games during the regular season from late March/early April until late October/early November. Given its long existence, we are interested in studying how the popularity of the sport has changed over time and how the popularity varies within each year as well.\nData\nWe are using Google search trend data for the term “MLB” from January 2004 to February 2022. The data gives us the popularity of the search term for each month in the time period. The values range from 0-100. A value of 100 represents the peak popularity of the search term. A value of 50 means the term is half as popular. A score of 0 means there was not enough data for the search popularity at that time.\nThe variables of interest are the month and the popularity of that given month. The variables were measured by Google using data from the amount of searches in the given month.\nMethods\nIn the original, un-transformed data, the trend cycles showed a large increase in variance over time, so we log transformed the data to reduce this increase in variance.\n\n\nmlb = read.csv(\"multiTimeline.csv\")\ncolnames(mlb) = c(\"popularity\")\nmlb = mlb[-1,, drop = FALSE]\nmlb = mlb %>%\n  mutate(log.popularity = log(as.numeric(popularity)),\n         Month = (1:nrow(mlb) - 1) %% 12 + 1,\n         Year = rep((1:nrow(mlb) - 1) %/% 12 + 2004),\n         Date = paste(Year, Month, 1 , sep = '-'),\n         Date = ymd(Date),\n         Date_dec = decimal_date(Date),\n         pandemic = if_else(Month %in% c(3, 4, 5, 6) & Year == 2020, 1, 0))\n\n\n\nTo estimate and model the trend, we used b-splines of varying degrees to try and accurately capture the general pattern of the data. We tested b-splines with degrees of 2, 3, and 4 to model the trend. All three estimates of the trend included a knot at 2020 as there is some irregularity in the data due to COVID that year. We selected the model that best captured the trend and led to less variability in the resulting residuals.\n\n\nmlb = mlb %>%\n  mutate(mlbTrend2 = predict(lm(log.popularity ~ bs( Date_dec, knots= c(2020), degree = 2), data = mlb)))\n\nmlb = mlb %>%\n  mutate(mlbTrend3 = predict(lm(log.popularity ~ bs( Date_dec, knots= c(2020), degree = 3), data = mlb)))\n\nmlb = mlb %>%\n  mutate(mlbTrend4 = predict(lm(log.popularity ~ bs( Date_dec, knots= c(2020), degree = 4), data = mlb)))\n\nmlb = mlb %>%\n  mutate(Detrend3 = log.popularity - mlbTrend3,\n         Detrend2 = log.popularity - mlbTrend2,\n         Detrend4 = log.popularity - mlbTrend4)\n\n\n\nTo model the seasonality, we used indicator variables for the month to estimate the monthly average deviations due to the seasonality. We also added an indicator for the months during the pandemic, as we noticed some different trends during this time period. By doing this we can plot the average monthly deviations and get a sense of the estimated seasonality of the data.\n\n\n# Estimating Seasonality\nlm.season <- lm(Detrend2 ~ factor(Month) + pandemic:I(Month==3) + pandemic:I(Month==4), data = mlb)\n  \nmlb <- mlb %>%\n  mutate(Season = predict(lm.season, newdata = mlb))\n\n\n\nAfter removing both the trend and seasonality using the 2nd degree b-splines and the month indicator variables, we are left with the remaining errors. To model the errors, we looked at the autocorrelation function using the astsa package of the errors to identify potential patterns that would indicate reasonable models to use. (Stoffer and Stoffer 2021) We ended up modeling the errors with an AR(15) model, an AR(1) model, and a seasonal AR(1) model. To decide between these models, we looked at the SARIMA output and compared the resulting acf plots and Ljung-Box tests, looking for acf plots that looked like white noise and higher values for the Ljung-Box test.\n\n\n# Fitting the errors\nmlb <- mlb %>%\n  mutate(Errors = Detrend2 - Season)\n\n# Autocorrelation\nlibrary(astsa)\n# acf2(mlb$Errors)\n\nerrorTS = ts(mlb$Errors, start = c(2004, 1), frequency = 12)\n\n# Error models:\n# sarima(errorTS, p = 15, d = 0, q = 0)\n# sarima(errorTS, p = 1, d = 0, q = 0)\n# sarima(errorTS, p = 1, d = 0, q = 0, P = 1, D = 0, S = 12)\n\n\n\nLastly, we combined the trend and seasonality models together, and then incorporated it with the seasonal AR(1) error model into a final SARIMA model and assessed its performance using similar methods as we did with the previous SARIMA models. Using this final model, we also did a 24 month forecast into the future to see what the model predicts will happen moving forward.\n\n\n# Forecasting set-up\nBknots = c(min(mlb$Date_dec),decimal_date(max(mlb$Date) %m+% months(24)))\n\ny = lm(log.popularity ~ bs(Date_dec, knots= c(2020), degree = 2,Boundary.knots =  Bknots) +factor(Month) + pandemic:I(Month==3) + pandemic:I(Month==4), data = mlb)\nX = model.matrix(y)[,-1]\n\nnewdata = data.frame(Date = max(mlb$Date) %m+% months(1:24)) %>% \n  mutate(Date_dec = decimal_date(Date), Month=month(Date), pandemic=0) \n\nnewX <- model.matrix(~bs(Date_dec, knots= c(2020), degree = 2,Boundary.knots =  Bknots) +factor(Month) + pandemic:I(Month==3) + pandemic:I(Month==4), newdata)[,-1]\n\n# Final SARIMA Model:\n# sarima(mlb_data, p = 1, d = 0, q = 0, P = 1, D = 0, S = 12, xreg = X)\n\n# Forecasting into the future:\n# sarima.for(mlb_data, 24, p = 1, d = 0, q = 0, P = 1, D = 0, S = 12, xreg = X,newxreg = newX)\n\n\n\nAdditionally, the following packages were used in our project to work with the data, make visualizations, and perform addition analyses: dplyr (Wickham et al. 2014), lubridate (Grolemund and Wickham 2013), stringr (Wickham and Wickham 2019), ggplot2 (Wickham, Chang, and Wickham 2016), splines (Bates, Venables, and Team 2011), and readr (Wickham et al. 2015).\nResults\nOur first step in modeling the data was to first model the trend. We wanted a model that would estimate the trend without being affected by the seasonality. A linear model was considered, but because of the pandemic affected season in 2020 leading to a drop in popularity, we decided to go with a b-spline. Using a spline allowed us to have more flexibility by fitting different parts of the data with different models.\n\n\n\nWe fit splines of degree 2,3, and 4 using a knot at January 2020. Each of the three did a good job of fitting the trend. The residual plots showed all the models did a good job of estimating the trend, with degree 4 doing the best. This, however, comes at the expense of having a more complex model. A more complex model leads to higher variance. Because we see that all 3 of the models estimate a very similar trend, we choose the least complex model to fit the rest of the data. This left us with a b-spline of degree 2 with a knot at January 2020 as our trend estimate.\n\n\n\nWe estimated the monthly average deviations due to seasonality by predicting the residuals as a function of the month, using the pandemic indicator variables. We added an indicator variable because we noticed some unusual trends due to the pandemic. We then plotted the average monthly deviations to get a sense of the estimated seasonality of the data. We see that the peak months for the log popularity is from April-October, with lows from November-March. in the residuals plot, we see that this method does best at removing the seasonality from ~2011-2015, while the other years have greater errors and variation.\nThe residual plot also shows the model does a poor job of modeling the seasonality in 2020 and 2021. In 2020 it severely overestimates the the seasonality and does the opposite in 2021.\n\n\n\nAfter removing both the trend and seasonality using the 2nd degree b-splines and the month indicator variables, we are left with the remaining errors. Looking at the autocorrelation function we see the ACF plot decays to 0, and the PACF drops to 0 after 1 lag, but is not 0 again around 13. This indicates that a seasonal AR(1) will do the best job of modeling the errors.\n\n\n     [,1]  [,2]  [,3] [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]\nACF  0.63  0.35  0.12 0.04 -0.09 -0.14 -0.19 -0.14 -0.09  0.10  0.30\nPACF 0.63 -0.08 -0.10 0.04 -0.16 -0.02 -0.08  0.03  0.02  0.22  0.25\n     [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\nACF   0.47  0.29  0.07 -0.05 -0.11 -0.16 -0.19 -0.20 -0.12 -0.03\nPACF  0.21 -0.31 -0.20 -0.02 -0.04  0.06  0.03  0.04  0.11  0.01\n     [,22] [,23] [,24] [,25]\nACF   0.13  0.30  0.36  0.26\nPACF  0.02  0.03 -0.04  0.04\n\nThe SARIMA function shows a seasonal AR(1) does an excellent job modeling the noise. The ACF of the residuals is all white noise, and the p values for the Ljung-Box statistic are all significantly above the threshold.\n\ninitial  value -1.421678 \niter   2 value -1.829822\niter   3 value -1.831444\niter   4 value -1.831692\niter   5 value -1.831714\niter   6 value -1.831808\niter   7 value -1.831956\niter   8 value -1.832009\niter   9 value -1.832016\niter  10 value -1.832018\niter  10 value -1.832018\niter  10 value -1.832018\nfinal  value -1.832018 \nconverged\ninitial  value -1.824807 \niter   2 value -1.824863\niter   3 value -1.824889\niter   4 value -1.824905\niter   5 value -1.824954\niter   6 value -1.824994\niter   7 value -1.825014\niter   8 value -1.825017\niter   9 value -1.825017\niter   9 value -1.825017\niter   9 value -1.825017\nfinal  value -1.825017 \nconverged\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n        ar1    sar1   xmean\n      0.616  0.5573  0.0106\ns.e.  0.053  0.0598  0.0592\n\nsigma^2 estimated as 0.02541:  log likelihood = 88.53,  aic = -169.05\n\n$degrees_of_freedom\n[1] 215\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.6160 0.0530 11.6325  0.0000\nsar1    0.5573 0.0598  9.3121  0.0000\nxmean   0.0106 0.0592  0.1788  0.8583\n\n$AIC\n[1] -0.775459\n\n$AICc\n[1] -0.7749445\n\n$BIC\n[1] -0.7133582\n\nWe forecasted the next two years using the seasonal AR(1) model with the estimates for the trend and seasonality. The model forecasted a small decrease in popularity each year for the next two years. This decrease makes sense because it is following the general shape of the trend model, with a continued decrease past 2021.\n\ninitial  value -1.455398 \niter   2 value -1.828628\niter   3 value -1.838249\niter   4 value -1.855558\niter   5 value -1.858660\niter   6 value -1.859781\niter   7 value -1.860697\niter   8 value -1.861772\niter   9 value -1.862144\niter  10 value -1.862430\niter  11 value -1.862693\niter  12 value -1.862842\niter  13 value -1.862886\niter  14 value -1.862899\niter  15 value -1.862906\niter  16 value -1.862910\niter  17 value -1.862912\niter  18 value -1.862914\niter  19 value -1.862915\niter  20 value -1.862915\niter  21 value -1.862916\niter  22 value -1.862916\niter  23 value -1.862916\niter  24 value -1.862916\niter  25 value -1.862916\niter  25 value -1.862916\niter  25 value -1.862916\nfinal  value -1.862916 \nconverged\ninitial  value -1.839426 \niter   2 value -1.841995\niter   3 value -1.843439\niter   4 value -1.844417\niter   5 value -1.845374\niter   6 value -1.846265\niter   7 value -1.846967\niter   8 value -1.847281\niter   9 value -1.847459\niter  10 value -1.847516\niter  11 value -1.847543\niter  12 value -1.847557\niter  13 value -1.847563\niter  14 value -1.847566\niter  15 value -1.847567\niter  16 value -1.847569\niter  17 value -1.847571\niter  18 value -1.847573\niter  19 value -1.847574\niter  20 value -1.847574\niter  21 value -1.847574\niter  21 value -1.847574\niter  21 value -1.847574\nfinal  value -1.847574 \nconverged\n\n$fit\n\nCall:\nstats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, \n    Q), period = S), xreg = xreg, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n    REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1    sar1  intercept\n      0.6319  0.5795     1.4129\ns.e.  0.0613  0.0599     0.1667\n      bs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)1\n                                                                   0.9454\ns.e.                                                               0.3180\n      bs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)2\n                                                                   1.1516\ns.e.                                                               0.2114\n      bs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)3\n                                                                   0.5667\ns.e.                                                               0.8085\n      factor(Month)2  factor(Month)3  factor(Month)4  factor(Month)5\n              0.1491          0.5695          1.4007          1.3159\ns.e.          0.0878          0.1136          0.1268          0.1338\n      factor(Month)6  factor(Month)7  factor(Month)8  factor(Month)9\n              1.4881          1.5561          1.3952          1.4172\ns.e.          0.1372          0.1382          0.1370          0.1336\n      factor(Month)10  factor(Month)11  factor(Month)12\n               1.6064           0.2003           0.1657\ns.e.           0.1268           0.1141           0.0895\n      pandemic:I(Month == 3)FALSE  pandemic:I(Month == 3)TRUE\n                          -1.0834                     -0.2020\ns.e.                       0.1422                      0.1375\n      pandemic:I(Month == 4)TRUE\n                         -0.4044\ns.e.                      0.1389\n\nsigma^2 estimated as 0.02423:  log likelihood = 93.44,  aic = -144.89\n\n$degrees_of_freedom\n[1] 198\n\n$ttable\n                                                                    Estimate\nar1                                                                   0.6319\nsar1                                                                  0.5795\nintercept                                                             1.4129\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)1   0.9454\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)2   1.1516\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)3   0.5667\nfactor(Month)2                                                        0.1491\nfactor(Month)3                                                        0.5695\nfactor(Month)4                                                        1.4007\nfactor(Month)5                                                        1.3159\nfactor(Month)6                                                        1.4881\nfactor(Month)7                                                        1.5561\nfactor(Month)8                                                        1.3952\nfactor(Month)9                                                        1.4172\nfactor(Month)10                                                       1.6064\nfactor(Month)11                                                       0.2003\nfactor(Month)12                                                       0.1657\npandemic:I(Month == 3)FALSE                                          -1.0834\npandemic:I(Month == 3)TRUE                                           -0.2020\npandemic:I(Month == 4)TRUE                                           -0.4044\n                                                                        SE\nar1                                                                 0.0613\nsar1                                                                0.0599\nintercept                                                           0.1667\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)1 0.3180\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)2 0.2114\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)3 0.8085\nfactor(Month)2                                                      0.0878\nfactor(Month)3                                                      0.1136\nfactor(Month)4                                                      0.1268\nfactor(Month)5                                                      0.1338\nfactor(Month)6                                                      0.1372\nfactor(Month)7                                                      0.1382\nfactor(Month)8                                                      0.1370\nfactor(Month)9                                                      0.1336\nfactor(Month)10                                                     0.1268\nfactor(Month)11                                                     0.1141\nfactor(Month)12                                                     0.0895\npandemic:I(Month == 3)FALSE                                         0.1422\npandemic:I(Month == 3)TRUE                                          0.1375\npandemic:I(Month == 4)TRUE                                          0.1389\n                                                                    t.value\nar1                                                                 10.3007\nsar1                                                                 9.6735\nintercept                                                            8.4744\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)1  2.9733\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)2  5.4467\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)3  0.7010\nfactor(Month)2                                                       1.6976\nfactor(Month)3                                                       5.0148\nfactor(Month)4                                                      11.0438\nfactor(Month)5                                                       9.8339\nfactor(Month)6                                                      10.8425\nfactor(Month)7                                                      11.2608\nfactor(Month)8                                                      10.1850\nfactor(Month)9                                                      10.6089\nfactor(Month)10                                                     12.6707\nfactor(Month)11                                                      1.7559\nfactor(Month)12                                                      1.8510\npandemic:I(Month == 3)FALSE                                         -7.6199\npandemic:I(Month == 3)TRUE                                          -1.4690\npandemic:I(Month == 4)TRUE                                          -2.9103\n                                                                    p.value\nar1                                                                  0.0000\nsar1                                                                 0.0000\nintercept                                                            0.0000\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)1  0.0033\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)2  0.0000\nbs(Date_dec, knots = c(2020), degree = 2, Boundary.knots = Bknots)3  0.4842\nfactor(Month)2                                                       0.0912\nfactor(Month)3                                                       0.0000\nfactor(Month)4                                                       0.0000\nfactor(Month)5                                                       0.0000\nfactor(Month)6                                                       0.0000\nfactor(Month)7                                                       0.0000\nfactor(Month)8                                                       0.0000\nfactor(Month)9                                                       0.0000\nfactor(Month)10                                                      0.0000\nfactor(Month)11                                                      0.0806\nfactor(Month)12                                                      0.0657\npandemic:I(Month == 3)FALSE                                          0.0000\npandemic:I(Month == 3)TRUE                                           0.1434\npandemic:I(Month == 4)TRUE                                           0.0040\n\n$AIC\n[1] -0.6646108\n\n$AICc\n[1] -0.6450514\n\n$BIC\n[1] -0.3385815\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul\n2022                   2.553591 3.828661 3.822476 3.937531 4.071877\n2023 1.972754 2.102584 2.544448 3.621640 3.571247 3.698445 3.793138\n2024 1.845217 1.969401                                             \n          Aug      Sep      Oct      Nov      Dec\n2022 3.940133 3.964095 4.077213 2.516225 2.051923\n2023 3.636725 3.647229 3.779863 2.270872 1.974346\n2024                                             \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun\n2022                     0.1556702 0.1841463 0.1943549 0.1982846\n2023 0.2008502 0.2008527 0.2204393 0.2277904 0.2306604 0.2317965\n2024 0.2325474 0.2325481                                        \n           Jul       Aug       Sep       Oct       Nov       Dec\n2022 0.1998322 0.2004468 0.2006917 0.2007894 0.2008284 0.2008440\n2023 0.2322486 0.2324288 0.2325008 0.2325295 0.2325410 0.2325456\n2024                                                            \n\nConclusions\nOur forecasting method showed some interesting results. It predicts a decrease in popularity for 2022 and 2023, even after an increase in popularity from 2020 to 2021. This is due to the COVID pandemic making us change the way we modeled the data. Modeling the trend of the popularity from 2004 to 2019 would be fairly easy to estimate and forecast as it was a fairly linear increase, but 2020 drastically affected the shape of the trend.\nWe had to use a b-spline to account for the decrease in popularity due to the pandemic. The spline did a good job modeling the trend. However, because the pandemic was so recent we had very limited data on how the popularity would rebound. Furthermore, splines are more variable on edge cases. Using the spline to forecast led to the prediction that popularity would decrease over the next few years. We cannot say for certain if our future predictions are good or not, but we believe them to be a conservative forecasting.\nOn the other hand, the model appears to do a good job of forecasting the seasonality. The general shape of the previous years was kept in the forecast. This is due in part to the use of an indicator variable for the pandemic. Using an indicator variable caused the drastically different 2020 year to not have as large of an affect on the shape.\nThe results show how much an event like the covid pandemic can affect statistical analysis. Because the pandemic shortened a season, it affected the distribution of the search popularity for 2020, and caused a decrease in popularity from 2019 to 2021, even thought there was a fairly strong upward trend from 2004 to 2019. Because of the sudden change and lack of data on how the popularity would recover, it made modeling and forecasting a very difficult task.\nOverall, given the contest of the data with the pandemic, our model does a very good job fitting the data. On the other hand, he use of a spline coupled with the uncertainty on the actual affect of the pandemic on the popularity leads to the conclusion that the forecasting method might not be the best. To create a better forecast, there are other factors we need to take into account. For example, the rebound of the popularity is closely tied with the economic rebound for the nation. In conclusion, the trend and seasonality are well estimated, but we need more data to accurately forecast.\nAcknowledgements\nWe would like to thank Brianna Heggeseth, our Correlated Data professor, who taught us the skills necessary to perform this analysis and also provided us with constructive feedback and oversight along the way. In addition, the resources found at https://bcheggeseth.github.io/CorrelatedData/ were used during this project.\n\n\n\nBates, MD, B Venables, and Maintainer R Core Team. 2011. “Package ‘Splines’.” R Version 2 (0).\n\n\nGrolemund, Maintainer Garrett, and Hadley Wickham. 2013. “Package ‘Lubridate’.”\n\n\nStoffer, David, and Maintainer David Stoffer. 2021. “Package ‘Astsa’.” Blood 8: 1.\n\n\nWickham, Hadley, Winston Chang, and Maintainer Hadley Wickham. 2016. “Package ‘Ggplot2’.” Create Elegant Data Visualisations Using the Grammar of Graphics. Version 2 (1): 1–189.\n\n\nWickham, Hadley, R Francois, L Henry, and K Müller. 2014. “Dplyr.” In useR! Conference.\n\n\nWickham, Hadley, Jim Hester, Romain Francois, R Core Team, Jukka Jylänki, Mikkel Jørgensen, and Maintainer Jim Hester. 2015. “Package ‘Readr’.”\n\n\nWickham, Hadley, and Maintainer Hadley Wickham. 2019. “Package ‘Stringr’.” CRAN.\n\n\n\n\n",
    "preview": "posts/Stat452_TS_miniProj/ts_preview.png",
    "last_modified": "2022-03-02T10:28:31-06:00",
    "input_file": {}
  },
  {
    "path": "posts/Stat454capstone/",
    "title": "Bayesian Statistics Capstone Project",
    "description": "Bayesian Finance: Modeling Earnings for S&P 500 Companies",
    "author": [],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nAbstract\nFinancial information, like stock market prices, are known to be notoriously hard to predict. We wanted to take a Bayesian approach to try and tackle a similar situation: predicting the future earnings of S&P 500 companies. In this project we seek to model future earnings using other financial information about a company, like previous earnings and sales. We explore a few Bayesian hierarchical models, as well as a bayesforcast model to try and identify one that can provide insight and better predictions for future company’s earnings.\nOur project report can be found at: https://nolan-meyer.github.io/bayesian-finance/\n\n\n\n",
    "preview": "posts/Stat454capstone/website_report_screenshot.png",
    "last_modified": "2021-12-13T10:43:20-06:00",
    "input_file": {}
  },
  {
    "path": "posts/Stat112Final/",
    "title": "Intro Data Science Final Project",
    "description": "Building a Soccer Database & Shiny App",
    "author": [],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\n\n\n\nIntroduction\nIntroduction to topic/data\nAll the members in our group play soccer, with three of us on Macalester’s varsity team. Thus we were incredibly interested in incorporating some sort of soccer aspect into our project. We also wanted it to be somewhat specific to our time here at Macalester. Finally, we wanted to challenge ourselves to build our own dataset, and so we did just that. We decided to track Macalester’s men’s soccer team and player statistics for the last few years using a shiny app. After deciding on this, we diverted our attention to finding the necessary data. All the data was available online, we just had to compile data from all seasons dating back to 2012.\nResearch Questions\nOur first research question was: -How can we best compare Macalester men’s soccer team statistics across different seasons? -This would include such statistics as goals scored, goals allowed, assists, shots, etc. -We could also track team performance in general by tracking wins, losses, and draws\nWe also wanted to add on an individual component as well: -What is the best way to compare different player’s individual statistics across different seasons? -Such statistics would include goals, assists, minutes logged, yellow/red cards, etc. -Can we compare players that played on the same team? -Can we compare players that never played on the same team? -Can we track players’ progression through their 4 year career?\nWe also wanted to make it aesthetically pleasing: -What is the most effective way to display this information? -Would it make sense to aggregate certain aspects of the project? Would it aid in delivering the information? -What is the best way to organize the information?\nData Collection\nData Sources\nThe data was pulled from the Macalester Athletics website, specifically the Men’s Soccer page. Within the page there is a tab for statistics, which displays team, individual, game-by-game, and miscellaneous data from 2012-2019. There is also data going back to 2001, but it is limited to game scores.\nCollection process\nTo collect the data, we created a shared google sheet where we copied the data from table format on the web page and pasted it onto our sheet. Some of the individual statistics were broken down into 2 separate variables to better account our ability to display numerical values. The team statistics required a pivot function which we did manually. There were also 4 variables that we calculated, which we thought provided useful additional insight; goals per 90 minutes, assists per 90 minutes, points per 90 minutes, and yellow cards per 90 minutes.\nOnce all the data cleaning was complete, we downloaded the files as .csv, and uploaded them to R. We had a separate file for individual and team statistics.\nManual\nShiny App Link\nGitHub Repo Link\nWelcome to the Macalester Men’s Soccer Database\nWhen you first open the shiny app, it will automatically open to the Player Data screen. At the top of the app, you will see 5 different tabs. Click on one to explore. The section below goes into depth on how to navigate each tab.\nPlayer Data\nThis page displays the raw data for each player.\nOn the top left corner there is a tab “Show 8 entries”. If you click on the number 8, you will see a drop down menu. From there you will be able to change the amount of players displayed on the screen at a time.\nOn the top right corner there is a search bar where you can search for specific players by typing in their name.\nAt the bottom right of the screen, you will see the amount of pages. Use the buttons to navigate in between pages.\nTaking a look at the table, you can see all the raw data for each player. To organize by an individual statistic, click on the variable on the top. The first click will show from lowest to highest, and another click will show from highest to lowest. Once you click on another variable, the previous organization is undone.\nAt the bottom right of the screen, you will see the amount of pages. Use the buttons to navigate in between pages.\nTeam Data\nThis page displays the raw data for each season’s team.\nOn the top left corner there is a tab “Show 8 entries”. If you click on the number 8, you will see a drop down menu. From there you will be able to change the amount of teams displayed on the screen at a time. Note: there is only data for 8 teams.\nOn the top right corner there is a search bar where you can search for specific teams by typing in their season’s year.\nAt the bottom right of the screen, you will see the amount of pages. Use the buttons to navigate in between pages.\nTaking a look at the table, you can see all the raw data for each team. To organize by an individual statistic, click on the variable on the top. The first click will show from lowest to highest, and another click will show from highest to lowest. Once you click on another variable, the previous organization is undone.\nAt the bottom right of the screen, you will see the amount of pages. Use the buttons to navigate in between pages.\nPlayer Time Series\nOn the left side of the screen there are dropdowns and a slider which are the inputs for the graph you wish to display. The first dropdown is to select the statistic that you want to look at, which includes all of the variables listed on the Player Data table. The following 3 dropdowns are to select the individual players you wish to compare. The slider filters the data to the seasons you wish to look at.\nOnce finished, click create plot and your plot will be displayed on the right side of the screen.\nThe plot, located on the right side of the screen, is interactive. Hover over the points to get more data about the data point. Additionally, you can click and drag on the plot to create a box over a specific section of the screen, this will zoom you into that section.\nAt the top right side of the plot are several buttons with more functions you can perform. Hovering over them will allow you to understand what they do, such as zooming in and out, as well as downloading the plot as a png.\nTeam Time Series\nOn the left side of the screen there are dropdowns and a slider which are the inputs for the graph you wish to display. The 3 dropdowns are to select the statistics that you want to look at, which includes all of the variables listed on the Team Data table. The slider filters the data to the seasons you wish to look at.\nOnce finished, click create plot and your plot will be displayed on the right side of the screen.\nThe plot, located on the right side of the screen, is interactive. Hover over the points to get more data about the data point. Additionally, you can click and drag on the plot to create a box over a specific section of the screen, this will zoom you into that section.\nAt the top right side of the plot are several buttons with more functions you can perform. Hovering over them will allow you to understand what they do, such as zooming in and out, as well as downloading the plot as a png.\nPlayer Scatter Plot\nOn the left side of the screen there are dropdowns and a slider which are the inputs for the graph you wish to display. The first 2 dropdowns are to select the statistics that you want to look at, which includes all of the variables listed on the Player Data table. The sliders filter the data to the seasons, minutes, and games played that you wish to look at\nOnce finished, click create plot and your plot will be displayed on the right side of the screen.\nThe plot, located on the right side of the screen, is interactive. Hover over the points to get more data about the data point. Additionally, you can click and drag on the plot to create a box over a specific section of the screen, this will zoom you into that section.\nAt the top right side of the plot are several buttons with more functions you can perform. Hovering over them will allow you to understand what they do, such as zooming in and out, as well as downloading the plot as a png.\n\n\n\n",
    "preview": "posts/Stat112Final/shinyappscreenshot.png",
    "last_modified": "2021-12-01T11:43:50-06:00",
    "input_file": {}
  }
]
